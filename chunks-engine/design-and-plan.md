# ğŸ“„ Chunks Engine â€” Design & Plan

## 1. Purpose

The Chunks Engine breaks government policy documents into **semantic chunks** â€” meaningful, self-contained segments of text that can be independently embedded, retrieved, and cited. Each chunk is tagged with rich metadata (state, department, effective date, expiry date, target demographic) enabling precise, filtered retrieval.

**Core Mission:** Transform long, complex government documents into RAG-ready chunks that maximize retrieval precision and minimize hallucination risk.

---

## 2. Capabilities

| Capability | Description |
|---|---|
| **Semantic Chunking** | Split documents by meaning, not just character count |
| **Metadata Tagging** | Auto-tag: state, department, effective date, expiry, demographic |
| **Hierarchical Chunking** | Maintain parent-child relationships between chunks |
| **Multi-Language Support** | Chunk Hindi, English, and regional language documents |
| **Overlap Management** | Configurable chunk overlap for context continuity |
| **Re-Chunking** | Auto re-chunk when policies are updated |
| **Deduplication** | Detect and merge duplicate/near-duplicate chunks |
| **Quality Scoring** | Score each chunk for completeness and coherence |
| **Async Ingestion** | Asynchronous pipeline for high-volume document processing |
| **Version Tracking** | Track which document version produced each chunk |

---

## 3. Architecture

### 3.1 Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input Sources                              â”‚
â”‚                                                              â”‚
â”‚  Policy Fetching â”‚ Gov Data Sync â”‚ Document Understanding    â”‚
â”‚  Engine          â”‚ Engine        â”‚ Engine                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ Raw documents (PDF, HTML, text)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Chunks Engine                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Pre-Processing Layer                     â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ PDF/HTML   â”‚  â”‚ Language   â”‚  â”‚ Section        â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Parser     â”‚  â”‚ Detector   â”‚  â”‚ Detector       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚ (heading/para) â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Chunking Strategy Layer                   â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Semantic       â”‚  â”‚ Configurable Parameters  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Chunker        â”‚  â”‚                          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                â”‚  â”‚ â€¢ chunk_size: 512 tokens  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Sentence     â”‚  â”‚ â€¢ overlap: 50 tokens     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   boundary     â”‚  â”‚ â€¢ min_chunk: 100 tokens   â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Section      â”‚  â”‚ â€¢ max_chunk: 1024 tokens  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   boundary     â”‚  â”‚ â€¢ strategy: semantic      â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Topic change â”‚  â”‚                          â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Metadata Extraction Layer                 â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ State      â”‚  â”‚ Date       â”‚  â”‚ Demographic    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Detector   â”‚  â”‚ Extractor  â”‚  â”‚ Tagger         â”‚  â”‚    â”‚
â”‚  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚                â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ NER for    â”‚  â”‚ Effective  â”‚  â”‚ Target group   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ state/dept â”‚  â”‚ & expiry   â”‚  â”‚ classification â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Output Layer                             â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Quality    â”‚  â”‚ Dedup      â”‚  â”‚ Event          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Scorer     â”‚  â”‚ Filter     â”‚  â”‚ Publisher      â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector       â”‚  â”‚ Event Bus  â”‚  â”‚ Raw Data     â”‚
    â”‚ Database     â”‚  â”‚            â”‚  â”‚ Store        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Data Models

### 4.1 Chunk Output

```json
{
  "chunk_id": "chk_uuid_v4",
  "document_id": "doc_uuid",
  "policy_id": "PM-KISAN-2024-v3",
  "chunk_index": 5,
  "total_chunks": 23,
  "parent_chunk_id": "chk_parent_uuid",
  
  "text": "Under PM-KISAN, all land-holding farmer families shall receive income support of Rs. 6,000 per year in three equal instalments...",
  "text_length_tokens": 487,
  "language": "en",
  
  "metadata": {
    "state": "CENTRAL",
    "ministry": "Agriculture and Farmers Welfare",
    "department": "Department of Agriculture",
    "effective_date": "2024-02-01",
    "expiry_date": null,
    "target_demographics": ["farmers", "land_holders"],
    "scheme_type": "direct_benefit_transfer",
    "section_heading": "Eligibility and Benefits",
    "source_url": "https://pmkisan.gov.in/guidelines_v3.pdf",
    "page_number": 4
  },
  
  "quality": {
    "completeness_score": 0.92,
    "coherence_score": 0.88,
    "standalone_readability": 0.85
  },
  
  "versioning": {
    "document_version": 3,
    "chunk_version": 1,
    "chunked_at": "2026-02-26T10:00:00Z",
    "chunking_model": "semantic_v2",
    "prev_chunk_id": null
  }
}
```

### 4.2 Chunking Configuration

```yaml
chunking_profiles:
  government_policy:
    strategy: semantic
    chunk_size_tokens: 512
    overlap_tokens: 50
    min_chunk_tokens: 100
    max_chunk_tokens: 1024
    split_on:
      - section_heading
      - numbered_clause
      - paragraph_break
    preserve:
      - tables
      - lists
      - definitions
      
  budget_speech:
    strategy: paragraph
    chunk_size_tokens: 300
    overlap_tokens: 30
    
  gazette_notification:
    strategy: semantic
    chunk_size_tokens: 400
    overlap_tokens: 40
```

---

## 5. Context Flow

```
Document arrives from Policy Fetching / Gov Data Sync Engine
    â”‚
    â”œâ”€â–º Pre-Processing
    â”‚       â”œâ”€â–º Parse format (PDF â†’ text, HTML â†’ text)
    â”‚       â”œâ”€â–º Detect language (en/hi/regional)
    â”‚       â”œâ”€â–º Detect sections (headings, clauses, tables)
    â”‚       â””â”€â–º Clean text (remove headers/footers, normalize whitespace)
    â”‚
    â”œâ”€â–º Chunking
    â”‚       â”œâ”€â–º Select chunking profile based on document type
    â”‚       â”œâ”€â–º Apply semantic splitting (sentence/section boundaries)
    â”‚       â”œâ”€â–º Ensure overlap between adjacent chunks
    â”‚       â”œâ”€â–º Maintain parent-child hierarchy
    â”‚       â””â”€â–º Validate chunk sizes (within min/max bounds)
    â”‚
    â”œâ”€â–º Metadata Extraction
    â”‚       â”œâ”€â–º NER: Extract state, department, ministry names
    â”‚       â”œâ”€â–º Date extraction: effective_date, expiry_date
    â”‚       â”œâ”€â–º Demographic tagging: target population segments
    â”‚       â””â”€â–º Source tracking: URL, page number, section heading
    â”‚
    â”œâ”€â–º Quality & Dedup
    â”‚       â”œâ”€â–º Score each chunk for completeness and coherence
    â”‚       â”œâ”€â–º Detect near-duplicate chunks (MinHash/SimHash)
    â”‚       â””â”€â–º Merge or flag duplicates
    â”‚
    â””â”€â–º Output
            â”œâ”€â–º Publish CHUNKS_CREATED event (Vector DB will embed)
            â”œâ”€â–º Log to Raw Data Store (processing audit)
            â””â”€â–º Update chunk registry
```

---

## 6. Event Bus Integration

| Event Consumed | Source | Action |
|---|---|---|
| `DOCUMENT_FETCHED` | Policy Fetching Engine | Start chunking pipeline |
| `DOCUMENT_UPDATED` | Gov Data Sync Engine | Re-chunk updated document |
| `DOCUMENT_PARSED` | Document Understanding Engine | Chunk structured extraction |

| Event Published | Consumers |
|---|---|
| `CHUNKS_CREATED` | Vector Database (embed and index) |
| `CHUNKS_UPDATED` | Vector Database (re-embed) |
| `CHUNKS_DELETED` | Vector Database (remove vectors) |
| `CHUNKING_FAILED` | Anomaly Detection, Admin Dashboard |

---

## 7. NVIDIA Stack Alignment

| Component | NVIDIA Tool | Purpose |
|---|---|---|
| Semantic chunking NLP | NeMo BERT | Sentence boundary detection, topic segmentation |
| NER extraction | NeMo NER | State, department, date extraction |
| Batch processing | RAPIDS cuDF | GPU-accelerated text processing |
| Language detection | NVIDIA Riva | Identify document language |

---

## 8. Scaling Strategy

| Scale Tier | Documents/Day | Strategy |
|---|---|---|
| **Tier 1** (MVP) | < 100 | Single worker, synchronous processing |
| **Tier 2** | 100 â€“ 10K | Async Kafka consumers, parallel chunking |
| **Tier 3** | 10K â€“ 100K | Worker pool with GPU-accelerated NLP |
| **Tier 4** | 100K+ | Distributed chunking cluster, streaming pipeline |

---

## 9. API Endpoints

| Method | Path | Description |
|---|---|---|
| `POST` | `/api/v1/chunks/process` | Submit document for chunking |
| `GET` | `/api/v1/chunks/document/{doc_id}` | Get all chunks for a document |
| `GET` | `/api/v1/chunks/{chunk_id}` | Get specific chunk |
| `POST` | `/api/v1/chunks/rechunk/{doc_id}` | Trigger re-chunking |
| `GET` | `/api/v1/chunks/stats` | Chunking statistics |
| `DELETE` | `/api/v1/chunks/document/{doc_id}` | Remove all chunks for a document |

---

## 10. Dependencies

| Dependency | Direction | Purpose |
|---|---|---|
| **Policy Fetching Engine** | Upstream | Raw documents to chunk |
| **Gov Data Sync Engine** | Upstream | Updated documents to re-chunk |
| **Document Understanding Engine** | Upstream | Structured document extractions |
| **Vector Database** | Downstream | Chunks to embed and index |
| **Event Bus** | Bidirectional | Event-driven processing |
| **Raw Data Store** | Downstream | Audit logging |

---

## 11. Technology Stack

| Layer | Technology |
|---|---|
| Runtime | Python 3.11 (FastAPI) |
| Chunking Library | LangChain TextSplitters + custom semantic splitter |
| NLP Models | NeMo BERT (sentence boundary, NER) |
| PDF Parsing | PyMuPDF (fitz) / pdfplumber |
| HTML Parsing | BeautifulSoup4 + trafilatura |
| Deduplication | datasketch (MinHash LSH) |
| Async Workers | Celery / ARQ |
| Event Bus | Apache Kafka |
| Containerization | Docker + Kubernetes |

---

## 12. Implementation Phases

| Phase | Milestone | Timeline |
|---|---|---|
| **Phase 1** | Basic fixed-size chunking, PDF/HTML parsing | Week 1-2 |
| **Phase 2** | Semantic chunking (sentence/section boundaries) | Week 3-4 |
| **Phase 3** | Metadata extraction (NER for state, dates) | Week 5-6 |
| **Phase 4** | Quality scoring, deduplication | Week 7-8 |
| **Phase 5** | Auto re-chunking on policy updates | Week 9-10 |
| **Phase 6** | Multilingual chunking, hierarchy support | Week 11-13 |

---

## 13. Success Metrics

| Metric | Target |
|---|---|
| Chunking throughput | > 100 documents/min |
| Chunk quality score (avg) | > 0.85 |
| Metadata extraction accuracy | > 90% |
| Duplicate detection rate | > 95% |
| Re-chunking latency | < 30s per document |
| Chunk-to-retrieval relevance | > 85% (measured via RAG eval) |
